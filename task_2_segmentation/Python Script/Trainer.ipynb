{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries & Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANSUMAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Error Handling\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "except ImportError:\n",
    "    from contextlib import contextmanager\n",
    "    @contextmanager\n",
    "    def autocast(enabled=True):\n",
    "        yield\n",
    "    class GradScaler:\n",
    "        def scale(self, loss):\n",
    "            return loss\n",
    "        def unscale_(self, optimizer):\n",
    "            pass\n",
    "        def step(self, optimizer):\n",
    "            optimizer.step()\n",
    "        def update(self):\n",
    "            pass\n",
    "\n",
    "# Set Current Working Directory\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation: JointRandomFlipRotate\n",
    "Define the JointRandomFlipRotate class to perform random horizontal flips and rotations on images and corresponding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointRandomFlipRotate:\n",
    "    def __init__(self, p_flip=0.5, degrees=15):\n",
    "        self.p_flip = p_flip\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Random horizontal flip\n",
    "        if random.random() < self.p_flip:\n",
    "            image = F.hflip(image)\n",
    "            mask = F.hflip(mask)\n",
    "        \n",
    "        # Random rotation\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        image = F.rotate(image, angle, fill=0)\n",
    "        mask = F.rotate(mask, angle, fill=0)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultrasound Segmentation Dataset\n",
    "Create a custom Dataset class to load ultrasound images and masks, apply joint and individual transforms, and return the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for segmentation\n",
    "class UltrasoundSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, joint_transform=None, transform_img=None, transform_mask=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        self.mask_files = sorted(os.listdir(masks_dir))\n",
    "        self.joint_transform = joint_transform\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_mask = transform_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.mask_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        # Apply joint augmentation if provided\n",
    "        if self.joint_transform is not None:\n",
    "            image, mask = self.joint_transform(image, mask)\n",
    "        \n",
    "        # Apply individual transforms if provided\n",
    "        if self.transform_img is not None:\n",
    "            image = self.transform_img(image)\n",
    "        if self.transform_mask is not None:\n",
    "            mask = self.transform_mask(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Transforms (resize and to-tensor)\n",
    "transform_img = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "transform_mask = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Update these paths as needed\n",
    "images_dir = project_root / \"Dataset\" / \"Images\"\n",
    "masks_dir = project_root / \"Dataset\" / \"Masks\"\n",
    "\n",
    "# Create dataset and loaders\n",
    "joint_augmentation = JointRandomFlipRotate(p_flip=0.5, degrees=15)\n",
    "dataset = UltrasoundSegmentationDataset(images_dir, masks_dir, joint_transform=joint_augmentation, \n",
    "                                        transform_img=transform_img, transform_mask=transform_mask)\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transforms and Loader Creation\n",
    "Define transforms for images and masks (resize and to-tensor), then create dataset instances. Split the dataset into training and validation sets and create DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms (resize and to-tensor)\n",
    "transform_img = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "transform_mask = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Update these paths as needed\n",
    "images_dir = project_root /  \"Dataset\" / \"Images\"\n",
    "masks_dir = project_root /  \"Dataset\" / \"Masks\"\n",
    "\n",
    "# Create dataset and loaders\n",
    "joint_augmentation = JointRandomFlipRotate(p_flip=0.5, degrees=15)\n",
    "dataset = UltrasoundSegmentationDataset(images_dir, masks_dir, joint_transform=joint_augmentation, \n",
    "                                        transform_img=transform_img, transform_mask=transform_mask)\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model Definition\n",
    "Implement the U-Net segmentation model architecture using PyTorch modules including encoders, decoders, a bottleneck, and the final convolution with sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (enc_conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (enc_conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (dec_conv2): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (dec_conv1): Sequential(\n",
      "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (final_conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc_conv1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc_conv2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        b = self.bottleneck(p2)\n",
    "        up2 = self.upconv2(b)\n",
    "        cat2 = torch.cat([up2, e2], dim=1)\n",
    "        d2 = self.dec_conv2(cat2)\n",
    "        up1 = self.upconv1(d2)\n",
    "        cat1 = torch.cat([up1, e1], dim=1)\n",
    "        d1 = self.dec_conv1(cat1)\n",
    "        out = self.final_conv(d1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "model = UNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimizer, and Scheduler\n",
    "Define the loss function (BCELoss), set up the Adam optimizer, and configure a StepLR scheduler for learning rate adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Set up the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Configure a StepLR scheduler for learning rate adjustments\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop and Model Saving\n",
    "Implement the training and validation loops, calculate average losses, and save the model weights when the validation loss improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss to a high value\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, epochs, device):\n",
    "    global best_val_loss\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, masks)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save model weights if validation loss improves\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), project_root / \"Model Weights\" / \"hypothesis_best_model_weights.pth\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 30\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
